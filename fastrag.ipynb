{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "huggingface-cli login\n",
    "```\n",
    "give the read token when prompted!\n",
    "then for windows:\n",
    "```\n",
    "Enable Developer Mode on Windows (recommended long-term solution):\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "updated jypyternotebook: \n",
    "```\n",
    "pip install ipywidgets\n",
    "```\n",
    "\n",
    "packages\n",
    "```\n",
    "\n",
    "!pip install datasets\n",
    "!pip install torch transformers llama_index tqdm \n",
    "!pip install einops\n",
    "!pip install qdrant_client\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bidhi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Bidhi\\.cache\\huggingface\\hub\\datasets--squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|██████████| 87599/87599 [00:00<00:00, 507134.39 examples/s]\n",
      "Generating validation split: 100%|██████████| 10570/10570 [00:00<00:00, 397783.74 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# # Step 1: Load the SQuAD dataset\n",
    "# dataset = load_dataset(\"squad\")\n",
    "\n",
    "# # Step 2: Extract unique contexts{set} from the dataset\n",
    "# data = [item[\"context\"] for item in dataset[\"train\"]]\n",
    "# texts = list(set(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of text list is 17.656 MB\n"
     ]
    }
   ],
   "source": [
    "# texts[0]\n",
    "# import sys\n",
    "# size = sum(sys.getsizeof(text) for text in texts) + sys.getsizeof(texts)  # Sum elements + list overhead\n",
    "# size_in_mb = size / (1024*1024)\n",
    "\n",
    "# print(f\"Size of text list is {size_in_mb:.3f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batch_iterate(lst, batch_size):\n",
    "    for i in range(0, len(lst), batch_size):\n",
    "        yield lst[i : i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "generateEmbedding"
    ]
   },
   "outputs": [],
   "source": [
    "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "# # from tqdm import tqdm\n",
    "# #for jupyternotebook\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# class EmbedData:\n",
    "\n",
    "#     def __init__(self, \n",
    "#                  embed_model_name=\"nomic-ai/nomic-embed-text-v1.5\",\n",
    "#                  batch_size=32):\n",
    "        \n",
    "#         self.embed_model_name = embed_model_name\n",
    "#         self.embed_model = self._load_embed_model()\n",
    "#         self.batch_size = batch_size\n",
    "#         self.embeddings = []\n",
    "\n",
    "#     def _load_embed_model(self):\n",
    "#         embed_model = HuggingFaceEmbedding(model_name=self.embed_model_name,\n",
    "#                                            trust_remote_code=True,\n",
    "#                                            cache_folder='./hf_cache')\n",
    "#         return embed_model\n",
    "\n",
    "    \n",
    "#     def generate_embedding(self, context):\n",
    "#         return self.embed_model.get_text_embedding_batch(context)\n",
    "    \n",
    "#     def embed(self, contexts):\n",
    "#         self.contexts = contexts\n",
    "        \n",
    "#         total_batches = (len(contexts) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "#         with tqdm(total=total_batches, desc=\"Embedding data\", unit=\"batch\") as pbar:\n",
    "#             for batch_context in batch_iterate(contexts, self.batch_size):\n",
    "#                 batch_embeddings = self.generate_embedding(batch_context)\n",
    "#                 self.embeddings.extend(batch_embeddings)\n",
    "#                 pbar.update(1)  \n",
    "\n",
    "\n",
    "# batch_size = 32\n",
    "\n",
    "# embeddata = EmbedData(batch_size=batch_size)\n",
    "\n",
    "# embeddata.embed(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"embeddings_data.pkl\", \"rb\") as d:\n",
    "    embeddata = pickle.load(d)\n",
    "\n",
    "embeddata[\"contexts\"][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "default_segment_number=5: This means the collection will start with 5 segments, which can help balance performance and memory efficiency. More segments allow parallelism but might slightly increase query latency due to searching multiple segments.\n",
    "\n",
    "The indexing_threshold parameter controls when Qdrant should enable HNSW indexing for faster queries.\n",
    "  If indexing_threshold is set to 0 → Qdrant immediately creates an HNSW index, no matter how few vectors exist.\n",
    "\n",
    "  If indexing_threshold is set to a higher value (e.g., 10000) → Qdrant will wait until there are at least 10,000 vectors before enabling HNSW indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ingesting in batches: 37it [00:15,  2.38it/s]                        \n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client import models\n",
    "from tqdm import tqdm \n",
    "\n",
    "def batch_iterate(lst, batch_size):\n",
    "    for i in range(0, len(lst), batch_size):\n",
    "        yield lst[i : i + batch_size]\n",
    "\n",
    "class QdrantVDB:\n",
    "\n",
    "    def __init__(self, collection_name, vector_dim=768, batch_size=512):\n",
    "        self.collection_name = collection_name\n",
    "        self.batch_size = batch_size\n",
    "        self.vector_dim = vector_dim\n",
    "\n",
    "    def define_client(self):\n",
    "        self.client = QdrantClient(url=\"http://localhost:6333\",\n",
    "                                   prefer_grpc=True)\n",
    "        #gRPC is a communication mechanism that offers faster interactions compared to HTTP.\n",
    "\n",
    "    def create_collection(self):\n",
    "            \n",
    "            if not self.client.collection_exists(collection_name=self.collection_name):\n",
    "\n",
    "                self.client.create_collection(collection_name=self.collection_name,\n",
    "                                            \n",
    "                                            vectors_config=models.VectorParams(\n",
    "                                                                size=self.vector_dim,\n",
    "                                                                distance=models.Distance.DOT, #dot product similarity \n",
    "                                                                on_disk=True), # whether to store the vectors on disk instead of memory, which optimizes memory usage for larger datasets.\n",
    "                                            \n",
    "                                            optimizers_config=models.OptimizersConfigDiff(\n",
    "                                                                                default_segment_number=5,\n",
    "                                                                                indexing_threshold=0)\n",
    "                )\n",
    "\n",
    "\n",
    "                \n",
    "    def ingest_data(self, embeddata):\n",
    "    \n",
    "        for batch_context, batch_embeddings in tqdm(zip(batch_iterate(embeddata[\"contexts\"], self.batch_size), \n",
    "                                                        batch_iterate(embeddata[\"embeddings\"], self.batch_size)), \n",
    "                                                    total=len(embeddata[\"contexts\"])//self.batch_size, \n",
    "                                                    desc=\"Ingesting in batches\"):\n",
    "        \n",
    "            self.client.upload_collection(collection_name=self.collection_name,\n",
    "                                        vectors=batch_embeddings,\n",
    "                                        payload=[{\"context\": context} for context in batch_context])\n",
    "\n",
    "            self.client.update_collection(collection_name=self.collection_name,\n",
    "                                    optimizer_config=models.OptimizersConfigDiff(indexing_threshold=20000)\n",
    "                                    )\n",
    "            \n",
    "    def create_collection_fast(self):\n",
    "        \n",
    "        if not self.client.collection_exists(collection_name=self.collection_name):\n",
    "\n",
    "            self.client.create_collection(collection_name=self.collection_name,\n",
    "                                          \n",
    "                                          vectors_config=models.VectorParams(\n",
    "                                                              size=self.vector_dim,\n",
    "                                                              distance=models.Distance.DOT,\n",
    "                                                              on_disk=True),\n",
    "                                          \n",
    "                                          optimizers_config=models.OptimizersConfigDiff(\n",
    "                                                                            default_segment_number=5,\n",
    "                                                                            indexing_threshold=0),\n",
    "                                          \n",
    "                                          quantization_config=models.BinaryQuantization(\n",
    "                                                        binary=models.BinaryQuantizationConfig(always_ram=True)),\n",
    "                                         )\n",
    "\n",
    "database = QdrantVDB(\"squad_collection\")\n",
    "database.define_client()\n",
    "database.create_collection()\n",
    "database.ingest_data(embeddata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from qdrant_client import models\n",
    "\n",
    "embed_model_name=\"nomic-ai/nomic-embed-text-v1.5\"\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "            model_name=embed_model_name,\n",
    "            trust_remote_code=True,\n",
    "            cache_folder=\"./hf_cache\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class Retriever:\n",
    "    def __init__(self, vector_db, embed_model ):\n",
    "        self.vector_db = vector_db\n",
    "        self.embed_model = embed_model\n",
    "\n",
    "    def search(self, query):\n",
    "        \"\"\"Search for similar embeddings in the Qdrant database.\"\"\"\n",
    "\n",
    "        query_embedding = embed_model.get_text_embedding(query)\n",
    "            \n",
    "        # Start the timer\n",
    "        start_time = time.time()\n",
    "        \n",
    "        result = self.vector_db.client.search(\n",
    "            collection_name=self.vector_db.collection_name,\n",
    "            \n",
    "            query_vector=query_embedding,\n",
    "            \n",
    "            search_params=models.SearchParams(\n",
    "                quantization=models.QuantizationSearchParams(\n",
    "                    ignore= True,  # this need to be false to use it with fast search aka binary quantization\n",
    "                    rescore=True,\n",
    "                    oversampling=2.0,\n",
    "                )\n",
    "            ),\n",
    "            \n",
    "            timeout=1000,\n",
    "        )\n",
    "        \n",
    "        # End the timer\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        print(f\"Execution time for the search: {elapsed_time:.4f} seconds\")\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bidhi\\AppData\\Local\\Temp\\ipykernel_564\\2666107564.py:16: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  result = self.vector_db.client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for the search: 0.3095 seconds\n",
      "[ScoredPoint(id='3c050027-3103-4ba0-9e8a-50ef612c08e4', version=906, score=0.5884716510772705, payload={'context': 'Static analysis techniques for software verification can be applied also in the scenario of query languages. In particular, the *Abstract interpretation framework has been extended to the field of query languages for relational databases as a way to support sound approximation techniques. The semantics of query languages can be tuned according to suitable abstractions of the concrete domain of data. The abstraction of relational database system has many interesting applications, in particular, for security purposes, such as fine grained access control, watermarking, etc.'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='4028fe72-e637-4d95-8dbe-3080d049a1ee', version=610, score=0.5884716510772705, payload={'context': 'Static analysis techniques for software verification can be applied also in the scenario of query languages. In particular, the *Abstract interpretation framework has been extended to the field of query languages for relational databases as a way to support sound approximation techniques. The semantics of query languages can be tuned according to suitable abstractions of the concrete domain of data. The abstraction of relational database system has many interesting applications, in particular, for security purposes, such as fine grained access control, watermarking, etc.'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='884cfb0d-6d3f-426f-b273-d78361c3131d', version=314, score=0.5884716510772705, payload={'context': 'Static analysis techniques for software verification can be applied also in the scenario of query languages. In particular, the *Abstract interpretation framework has been extended to the field of query languages for relational databases as a way to support sound approximation techniques. The semantics of query languages can be tuned according to suitable abstractions of the concrete domain of data. The abstraction of relational database system has many interesting applications, in particular, for security purposes, such as fine grained access control, watermarking, etc.'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='b0ec5337-9e74-431f-b21d-107d59c8bacb', version=18, score=0.5884716510772705, payload={'context': 'Static analysis techniques for software verification can be applied also in the scenario of query languages. In particular, the *Abstract interpretation framework has been extended to the field of query languages for relational databases as a way to support sound approximation techniques. The semantics of query languages can be tuned according to suitable abstractions of the concrete domain of data. The abstraction of relational database system has many interesting applications, in particular, for security purposes, such as fine grained access control, watermarking, etc.'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='bfc7f99d-9aa5-4fbe-9d24-cbc802c4aead', version=824, score=0.5657821893692017, payload={'context': '\\nXML databases are a type of structured document-oriented database that allows querying based on XML document attributes. XML databases are mostly used in enterprise database management, where XML is being used as the machine-to-machine data interoperability standard. XML database management systems include commercial software MarkLogic and Oracle Berkeley DB XML, and a free use software Clusterpoint Distributed XML/JSON Database. All are enterprise software database platforms and support industry standard ACID-compliant transaction processing with strong database consistency characteristics and high level of database security.'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='9d07fd38-acc4-4747-87d8-bae69020966e', version=232, score=0.5657821893692017, payload={'context': '\\nXML databases are a type of structured document-oriented database that allows querying based on XML document attributes. XML databases are mostly used in enterprise database management, where XML is being used as the machine-to-machine data interoperability standard. XML database management systems include commercial software MarkLogic and Oracle Berkeley DB XML, and a free use software Clusterpoint Distributed XML/JSON Database. All are enterprise software database platforms and support industry standard ACID-compliant transaction processing with strong database consistency characteristics and high level of database security.'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='d34fc34e-8e39-4973-823e-4bbc0ba1e790', version=1120, score=0.5657821893692017, payload={'context': '\\nXML databases are a type of structured document-oriented database that allows querying based on XML document attributes. XML databases are mostly used in enterprise database management, where XML is being used as the machine-to-machine data interoperability standard. XML database management systems include commercial software MarkLogic and Oracle Berkeley DB XML, and a free use software Clusterpoint Distributed XML/JSON Database. All are enterprise software database platforms and support industry standard ACID-compliant transaction processing with strong database consistency characteristics and high level of database security.'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='f39796dc-6cf4-4b59-96ce-9aebbbe11609', version=528, score=0.5657821893692017, payload={'context': '\\nXML databases are a type of structured document-oriented database that allows querying based on XML document attributes. XML databases are mostly used in enterprise database management, where XML is being used as the machine-to-machine data interoperability standard. XML database management systems include commercial software MarkLogic and Oracle Berkeley DB XML, and a free use software Clusterpoint Distributed XML/JSON Database. All are enterprise software database platforms and support industry standard ACID-compliant transaction processing with strong database consistency characteristics and high level of database security.'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='948f6bf4-7f11-4ad2-8489-383e21607ba5', version=576, score=0.5460050106048584, payload={'context': 'Codd\\'s paper was picked up by two people at Berkeley, Eugene Wong and Michael Stonebraker. They started a project known as INGRES using funding that had already been allocated for a geographical database project and student programmers to produce code. Beginning in 1973, INGRES delivered its first test products which were generally ready for widespread use in 1979. INGRES was similar to System R in a number of ways, including the use of a \"language\" for data access, known as QUEL. Over time, INGRES moved to the emerging SQL standard.'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='0a53ecc0-c4f3-4868-aac5-12d414f58075', version=872, score=0.5460050106048584, payload={'context': 'Codd\\'s paper was picked up by two people at Berkeley, Eugene Wong and Michael Stonebraker. They started a project known as INGRES using funding that had already been allocated for a geographical database project and student programmers to produce code. Beginning in 1973, INGRES delivered its first test products which were generally ready for widespread use in 1979. INGRES was similar to System R in a number of ways, including the use of a \"language\" for data access, known as QUEL. Over time, INGRES moved to the emerging SQL standard.'}, vector=None, shard_key=None, order_value=None)]\n"
     ]
    }
   ],
   "source": [
    "results = Retriever(database, embed_model).search(\"Sample query\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "pip install llama-index qdrant_client torch transformers\n",
    "\n",
    "pip install llama-index-embeddings-huggingface\n",
    "\n",
    "pip install llama-index-llms-ollama\n",
    "\n",
    "pip install llama-index-vector-stores-qdrant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "class RAG:\n",
    "\n",
    "    def __init__(self,\n",
    "                 retriever,\n",
    "                 llm_name=\"llama3.2:1b\"):\n",
    "        \n",
    "        self.llm_name = llm_name\n",
    "        self.llm = self._setup_llm()\n",
    "        self.retriever = retriever\n",
    "        self.qa_prompt_tmpl_str = \"\"\"Context information is below.\n",
    "                                     ---------------------\n",
    "                                     {context}\n",
    "                                     ---------------------\n",
    "                                     \n",
    "                                     ---------------------\n",
    "                                     Query: {query}\n",
    "                                     ---------------------\n",
    "                                     \n",
    "                                     Answer: \"\"\"\n",
    "        \n",
    "    def _setup_llm(self):\n",
    "            return Ollama(model=self.llm_name)\n",
    "        \n",
    "    def generate_context(self, query):\n",
    "    \n",
    "            result = self.retriever.search(query)\n",
    "            context = [dict(data) for data in result]\n",
    "            combined_prompt = []\n",
    "\n",
    "            for entry in context:\n",
    "                context = entry[\"payload\"][\"context\"]\n",
    "\n",
    "                combined_prompt.append(context)\n",
    "\n",
    "            return \"\\n\\n---\\n\\n\".join(combined_prompt)\n",
    "        \n",
    "    def query(self, query):\n",
    "            context = self.generate_context(query=query)\n",
    "            \n",
    "            prompt = self.qa_prompt_tmpl_str.format(context=context, query=query)\n",
    "            \n",
    "            response = self.llm.complete(prompt)\n",
    "            \n",
    "            return dict(response)['text']\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Retriever(database, embeddata)\n",
    "\n",
    "rag = RAG(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for the search: 0.0079 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bidhi\\AppData\\Local\\Temp\\ipykernel_564\\3344487341.py:16: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  result = self.vector_db.client.search(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Airports may also contain premium and VIP services. The premium and VIP services may include express check-in and dedicated check-in counters. These services are usually reserved for First and Business class passengers, premium frequent flyers, and members of the airline's clubs. Premium services may sometimes be open to passengers who are members of a different airline's frequent flyer program. This can sometimes be part of a reciprocal deal, as when multiple airlines are part of the same alliance, or as a ploy to attract premium customers away from rival airlines.\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Retriever(database, embeddata).search(\"Sample query\")[0]\n",
    "###\n",
    "embeddata[\"contexts\"][11095]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bidhi\\AppData\\Local\\Temp\\ipykernel_564\\2666107564.py:16: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  result = self.vector_db.client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for the search: 0.0038 seconds\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The text does not specifically state the type of passengers that premium and VIP services are reserved for. However, it can be inferred that they are typically intended for First and Business class passengers, as well as members of airline's clubs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "query = \"\"\"The premium and VIP services in Airports\n",
    "           are reserved for which type of passengers?\"\"\"\n",
    "\n",
    "answer = rag.query(query)\n",
    "\n",
    "display(Markdown(str(answer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
